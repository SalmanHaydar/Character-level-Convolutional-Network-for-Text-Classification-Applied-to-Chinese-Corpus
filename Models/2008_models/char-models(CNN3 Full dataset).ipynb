{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "import re\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import text\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# 生成的 word vector 的 dimension\n",
    "maxlen = 1041\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz0123456789,.!? '\n",
    "datatrain = pd.read_csv(\"train.csv\", header=0)\n",
    "datatest = pd.read_csv(\"test.csv\", header=0)\n",
    "\n",
    "\n",
    "chars = set(alphabet)\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "\n",
    "# 创建 len(docs)个, 1 * maxlen 的矩阵\n",
    "X_train = np.ones((datatrain.shape[0], maxlen), dtype = np.int64) * 0\n",
    "\n",
    "docs = []\n",
    "labels = []\n",
    "\n",
    "print('zipping the data:')\n",
    "epoch = 0\n",
    "for cont,title,label in zip(datatrain.content, datatrain.title, datatrain.classes):\n",
    "    content = title + cont\n",
    "    content = re.sub(\"[^a-z0-9\\,\\.\\!\\?]\", \" \", content)\n",
    "    docs.append(content)\n",
    "    label = label - 1\n",
    "    labels.append(label)\n",
    "    epoch = epoch + 1\n",
    "    if (epoch % 20000 == 0):\n",
    "        print('zipping the training data:', epoch)\n",
    "print('Success!')\n",
    "\n",
    "\n",
    "print('There are training set:', datatrain.shape[0])\n",
    "\n",
    "\n",
    "print('Doing one hot encoding:')\n",
    "    # One-Hot encoding 另外应该是反过来进行 encode 的,,稀疏部分用0代替\n",
    "for i, doc in enumerate(docs):\n",
    "    # 倒着数后面的maxlen个数字,但是输出顺序不变\n",
    "    for t, char in enumerate(doc[-maxlen:]):\n",
    "                X_train[i, (maxlen-1-t)] = char_indices[char]\n",
    "print('Success!')\n",
    "\n",
    "\n",
    "Y_train = np.array(labels)\n",
    "\n",
    "print('Convert class vector to binary class matrix (for use with categorical_crossentropy)')\n",
    "nb_classes = 5\n",
    "print(nb_classes, 'classes in the dataset')\n",
    "Y_train = np_utils.to_categorical(Y_train, nb_classes)\n",
    "print('Success!')\n",
    "\n",
    "\n",
    "X_test = np.ones((datatest.shape[0], maxlen), dtype = np.int64) * 0\n",
    "docs = []\n",
    "labels = []\n",
    "\n",
    "print('zipping the test data:')\n",
    "epoch = 0\n",
    "for cont,title,label in zip(datatest.content, datatest.title, datatest.classes):\n",
    "    content = title + cont\n",
    "    content = re.sub(\"[^a-z0-9\\,\\.\\!\\?]\", \" \", content)\n",
    "    docs.append(content)\n",
    "    label = label - 1\n",
    "    labels.append(label)\n",
    "    epoch = epoch + 1\n",
    "    if (epoch % 20000 == 0):\n",
    "        print('zipping the test data:', epoch)\n",
    "print('Success!')\n",
    "\n",
    "print('There are test set:', datatest.shape[0])\n",
    "\n",
    "print('Doing one hot encoding:')\n",
    "    # One-Hot encoding 另外应该是反过来进行 encode 的,,稀疏部分用-1代替\n",
    "for i, doc in enumerate(docs):\n",
    "    # 倒着数后面的maxlen个数字,但是输出顺序不变\n",
    "    for t, char in enumerate(doc[-maxlen:]):\n",
    "                X_test[i, (maxlen-1-t)] = char_indices[char]\n",
    "print('Success!')\n",
    "\n",
    "Y_test = np.array(labels)\n",
    "\n",
    "print('Convert class vector to binary class matrix (for use with categorical_crossentropy)')\n",
    "nb_classes = 5\n",
    "print(nb_classes, 'classes in the dataset')\n",
    "Y_test = np_utils.to_categorical(Y_test, nb_classes)\n",
    "print('Success!')\n",
    "\n",
    "print(\"All of the pre-processde work is done.\")\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(input_dim = 41, output_dim = 50, input_length = maxlen, init = 'he_normal', W_regularizer=l2(0.01)) )\n",
    "\n",
    "# we add a Convolution1D, which will learn nb_filter\n",
    "# word group filters of size filter_length:\n",
    "model.add(Convolution1D(nb_filter = 128, filter_length = 3, W_regularizer=l2(0.01),  init = 'he_normal', border_mode='same', activation='relu', subsample_length=1))\n",
    "\n",
    "model.add(Convolution1D(nb_filter = 128, filter_length = 3, W_regularizer=l2(0.01),  init = 'he_normal', border_mode='same', activation='relu', subsample_length=1))\n",
    "\n",
    "model.add(Convolution1D(nb_filter = 128, filter_length = 3, W_regularizer=l2(0.01),  init = 'he_normal', border_mode='same', activation='relu', subsample_length=1))\n",
    "\n",
    "# we use max pooling:\n",
    "model.add(MaxPooling1D(pool_length = model.output_shape[1]))\n",
    "#model.add(MaxPooling1D(pool_length = 2))\n",
    "#print(model.output_shape[1], \"pooling shape\")\n",
    "# We flatten the output of the conv layer,\n",
    "# so that we can add a vanilla dense layer:\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(100))\n",
    "#model.add(Dropout(0.1))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "checkpointers = ModelCheckpoint(\"parameters/weights.{epoch:02d}-{val_acc:.4f}.hdf5\", monitor='val_acc', verbose=0, save_best_only=False, mode='auto')\n",
    "\n",
    "#model.load_weights(\"parameters/weights.39-0.32.hdf5\")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size = 128, nb_epoch = 20, validation_data=(X_test, Y_test), callbacks = [checkpointers])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
