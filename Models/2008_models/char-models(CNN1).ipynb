{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 41\n",
      "total dataset size: 60000\n",
      "zipping the data:\n",
      "zipping the data: 10000\n",
      "zipping the data: 20000\n",
      "zipping the data: 30000\n",
      "zipping the data: 40000\n",
      "zipping the data: 50000\n",
      "zipping the data: 60000\n",
      "Success!\n",
      "2792.9135\n",
      "Doing one hot encoding:\n",
      "Success!\n",
      "Randomize the dataset:\n",
      "Success!\n",
      "Spilt the dataset into train/test:\n",
      "Success!\n",
      "There are training set: 48000\n",
      "There are test set: 12000\n",
      "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
      "5 classes\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda, Flatten\n",
    "from keras.layers import Embedding, LSTM\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "import re\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import text\n",
    "from keras.regularizers import l2, activity_l2\n",
    "\n",
    "# 生成的 word vector 的 dimension\n",
    "maxlen = 1000\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz0123456789,.!? '\n",
    "data = pd.read_csv(\"test.csv\", header=0)\n",
    "\n",
    "\n",
    "chars = set(alphabet)\n",
    "print('total chars:', len(chars))\n",
    "# 在做 one-hot coding dict 中对应i = 1 时, c 是第一个字符...如此类推 两个 dict 只是 key-value 调换过来了\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# 创建 len(docs)个, 1 * maxlen 的矩阵\n",
    "X = np.ones((data.shape[0], maxlen), dtype = np.int64) * 0\n",
    "\n",
    "print('total dataset size:', data.shape[0])\n",
    "\n",
    "docs = []\n",
    "labels = []\n",
    "\n",
    "print('zipping the data:')\n",
    "epoch = 0\n",
    "avg = 0\n",
    "for cont,title,label in zip(data.content, data.title, data.classes):\n",
    "    content = title + cont\n",
    "    \n",
    "    content = re.sub(\"[^a-z0-9\\,\\.\\!\\?]\", \" \", content)\n",
    "    avg = avg + len(content)\n",
    "    docs.append(content)\n",
    "    label = label - 1\n",
    "    labels.append(label)\n",
    "    epoch = epoch + 1\n",
    "    if (epoch % 10000 == 0):\n",
    "        print('zipping the data:', epoch)\n",
    "print('Success!') \n",
    "\n",
    "print(avg/60000)\n",
    "\n",
    "print('Doing one hot encoding:')\n",
    "    # One-Hot encoding 另外应该是反过来进行 encode 的,,稀疏部分用-1代替\n",
    "for i, doc in enumerate(docs):\n",
    "    # 倒着数后面的maxlen个数字,但是输出顺序不变\n",
    "    for t, char in enumerate(doc[-maxlen:]):\n",
    "                X[i, (maxlen-1-t)] = char_indices[char]    \n",
    "print('Success!')    \n",
    "\n",
    "y = np.array(labels)   \n",
    "\n",
    "print('Randomize the dataset:')    \n",
    "ids = np.arange(len(X))\n",
    "np.random.shuffle(ids)\n",
    "X = X[ids]\n",
    "y = y[ids]\n",
    "print('Success!')   \n",
    "\n",
    "margin_size = int(data.shape[0] * 0.8)\n",
    "print('Spilt the dataset into train/test:') \n",
    "\n",
    "X_train = X[:margin_size]\n",
    "X_test = X[margin_size:]\n",
    "\n",
    "y_train = y[:margin_size]\n",
    "y_test = y[margin_size:]\n",
    "print('Success!')   \n",
    "\n",
    "print('There are training set:', margin_size)\n",
    "print('There are test set:', data.shape[0] - margin_size) \n",
    "\n",
    "print('Convert class vector to binary class matrix (for use with categorical_crossentropy)')\n",
    "nb_classes = 5 \n",
    "print(nb_classes, 'classes')\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "print('Success!')   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1967\n",
      "[36 23 28 37 22 23  9  7  4  9 12 36 36 35  9  6 36 22 20  4  9 21 36 22  5\n",
      " 26 25 36 35 26 18 36 36 36 36 36 22 36 15 36 20  9 11 36 37 22 20 23 40 36\n",
      " 35  9 40 36 37 22  5  4 10 36 35 23 27 36 35  9 13  7 36 23  5  4 10 36 20\n",
      " 28 11 36 22 35 26 27 36 20  4  2 36  3 23 28  7 36 20 23 27 36  9  5  4 18\n",
      " 36 37 22  5 26 13 18 36 22 35 26 27 36  3 23 29 36 14 36  5 28 40 36 23  3\n",
      "  4  9  6 36 35  9 40 36  5 28 13 36 37 22  5 23 10 36 37 22 35 23  0 36  9\n",
      " 20  4 37 36 37 22 20  9 10 36  5  9 13  7 36 25 35 26 36 14 36  5 28 40 36\n",
      " 23  3  4  9  6 36 35  9 40 36 20  9  7 36 37 22 20 23 37 36 37 22 35 23  0\n",
      " 36  9 20  4 37 36 37 22 20  9 10 36  5 28 21 36 26 40 36 35 26 28  6 36  3\n",
      " 23 29 36 14 36 35  9 40 36 37 22  5  4 10 36 35 23 27 36 35  9 13  7 36  5\n",
      "  9 13  7 36  3  9  2 36  9 35 26 27 36  5  4 28 39 36  9 35  4  1 36  9  5\n",
      " 26 29 36 36 36 37 22  3 26 27 36 35 23 28 37 36 37 22 35  4 10 36 36 22 36\n",
      " 30 36 35  9 40 36 37 22  5  4 10 36 22  3 26 18 36  5  9 13  7 36  3  9  2\n",
      " 36  9 35 26 27 36  5  4 28 39 36  9 35  4  1 36  9  5 26 29 36  9 35  4  1\n",
      " 36 22  5  4 39 36  3  9 22 36  9  5  4 18 36 36 36  3 26 13 18 36  5  9  6\n",
      " 36 36 22 36 15 36  3  9  2 36  9 35 26 27 36 35  9 40 36 20 28 13  7 36  5\n",
      " 26  2 36 37 22  3 23 13 18 36 20  9 10 36  5  9 13  7 36  5 28  9  6 36 22\n",
      " 35  4 28  8 36 22 35  4 29 36 26 27 36  5  4 22 36 14 36 22  5  4 39 36  5\n",
      " 28 40 36 23  3  4  9  6 36 35  9 40 36 23  5  4 37 36 37 22  3  4 28 37 36\n",
      "  9  5  4 27 36  3  9 10 36  5  9 13  7 36  3 23 28 37 36 35 28 25 36 14 36\n",
      "  9 35 26 29 36 20  9  7 36  5 26 37 36 22  5  4 28 13 36 35  9 13  7 36 35\n",
      "  9  8 36 14 36 22 20  4 28 37 36 35 28 29 36  3  9  2 36  9 35 26 27 36  5\n",
      "  4 28 39 36 35 26 13 36  5  9  7 36 22  5  4 39 36  5 28 29 36  5 26 10 36\n",
      "  5 28 29 36 37 22 20  4 13  7 36  3  9 18 36 22  5  4  9 40 36 36 36 22 35\n",
      " 26 25 36  5 26 37 36  5  9 28 40 36 22 35 26 25 36  5 26 37 36 36 36 35 36\n",
      " 36 35  9 40 36  5  9 13  7 36 37 22 35 23  0 36  3 23 28  7 36  5 28  8 36\n",
      "  5  9 10 36 36 36 36 36 36 37 22  3 26 27 36 35 23 28 37 36 37 22 35  4 10\n",
      " 36 36 22 36 30 36 35  4 27 36 22 35  4  9  1 36 22 20  4 28 37 36 28  3 23\n",
      " 10 36  9 35  4  2 36 37 22 35  9 38 36  3  9  2 36  9 35 26 27 36  5  4 28\n",
      " 39 36  3 28 10 36  5 28 29 36 37 22 20  4 13  7 36  3  9 18 36 22  5  4  9\n",
      " 40 36 14 36  5 28  8 36  5  9 10 36 36 36 36 36  9  5  4 18 36 37 22  5  4\n",
      " 17 36  5  9  1 36 37 22 20  9  6 36 20 23 28 40 36 22  3 26 13 36  3  4 21\n",
      " 36 22  5  4  9 11 36  9  5  4 18 36 36 36 36 36  9  5  4 18 36 22  5  4  9\n",
      " 11 36 36 36  3 26 13 18 36  5  9  6 36 36 22 36 36  3  9  2 36  9 35 26 27\n",
      " 36  5  9 13  7 36  3 26 10 36  5 28 29 36 37 22 20  4 13  7 36  3  9 18 36\n",
      " 22  5  4  9 40 36 36 22 36 15 36 35  9 40 36 35 28  9  1 36 23  3  4 21 36\n",
      " 35 28 29 36 23 35  4 13 36  3  9 10 36  3 26 39 36  5  9 13  7 36  5 23 28\n",
      " 18 36 35 26 13 36 35  9 40 36  3  9  2 36  9 35 26 27 36  5  4 28 39 36 14\n",
      " 36 22  5  4  9 11 36  9  5  4 18 36 36 36 36 36 28  3 23 10 36 28  5 23 10\n",
      " 36 14 36  5  9 13  7 36  9  5 26 29 36 35 28 10 36 35  4 28 13 36 28  3 23]\n",
      "48000000\n"
     ]
    }
   ],
   "source": [
    "print(len(content))\n",
    "print((X_train[0]))\n",
    "print(X_train.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding\n",
      "(None, 1000)\n",
      "(None, 1000, 50)\n",
      "Convolution1D\n",
      "(None, 998, 250)\n",
      "MaxPooling1D\n",
      "(None, 1, 250)\n",
      "Flatten\n",
      "(None, 250)\n",
      "Dense\n",
      "(None, 100)\n",
      "softmax\n",
      "(None, 5)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_11 (Embedding)         (None, 1000, 50)      2050        embedding_input_11[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_11 (Convolution1D) (None, 998, 250)      37750       embedding_11[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_11 (MaxPooling1D)   (None, 1, 250)        0           convolution1d_11[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_11 (Flatten)             (None, 250)           0           maxpooling1d_11[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_21 (Dense)                 (None, 100)           25100       flatten_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)             (None, 100)           0           dense_21[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_21 (Activation)       (None, 100)           0           dropout_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_22 (Dense)                 (None, 5)             505         activation_21[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_22 (Activation)       (None, 5)             0           dense_22[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 65405\n",
      "____________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/HUANGWEIJIE/anaconda/lib/python3.5/site-packages/tensorflow/python/ops/gradients.py:89: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/10\n",
      "21408/48000 [============>.................] - ETA: 1281s - loss: 1.5340 - acc: 0.4733"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-0ab7515dfe6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parameters/para.hdf5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/HUANGWEIJIE/anaconda/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m                               \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m                               sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/HUANGWEIJIE/anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1080\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/HUANGWEIJIE/anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[1;32m    799\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/HUANGWEIJIE/anaconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mupdated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/HUANGWEIJIE/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 372\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    373\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/HUANGWEIJIE/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 636\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    637\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m       \u001b[0;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/HUANGWEIJIE/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    706\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 708\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    709\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/HUANGWEIJIE/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    713\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/HUANGWEIJIE/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    695\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    696\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(input_dim = 41, output_dim = 50, input_length = maxlen, init = 'he_normal', W_regularizer=l2(0.01), dropout = 0.2))\n",
    "\n",
    "print(\"Embedding\")\n",
    "print(model.input_shape)\n",
    "print(model.output_shape)\n",
    "\n",
    "\n",
    "# we add a Convolution1D, which will learn nb_filter\n",
    "# word group filters of size filter_length:\n",
    "model.add(Convolution1D(nb_filter = 250, filter_length = 3, border_mode='valid', activation='relu', subsample_length=1))\n",
    "\n",
    "print(\"Convolution1D\")\n",
    "print(model.output_shape)\n",
    "\n",
    "# we use max pooling:\n",
    "model.add(MaxPooling1D(pool_length = model.output_shape[1]))\n",
    "\n",
    "print(\"MaxPooling1D\")\n",
    "print(model.output_shape)\n",
    "\n",
    "\n",
    "# We flatten the output of the conv layer,\n",
    "# so that we can add a vanilla dense layer:\n",
    "model.add(Flatten())\n",
    "print(\"Flatten\")\n",
    "print(model.output_shape)\n",
    "\n",
    "# 这个应该相当于fully connected layer吧\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "print(\"Dense\")\n",
    "print(model.output_shape)\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a softmax:\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "print(\"softmax\")\n",
    "print(model.output_shape)\n",
    "\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, batch_size = 32, nb_epoch = 10, validation_data=(X_test, Y_test))\n",
    "model.save_weights(\"parameters/para.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
