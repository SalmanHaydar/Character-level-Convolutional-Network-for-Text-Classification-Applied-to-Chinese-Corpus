{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "File b'new_CSV_Data/raw_chinese_train.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7a893f314723>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mmaxlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m250\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mdatatrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"new_CSV_Data/raw_chinese_train.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mdatatest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"new_CSV_Data/raw_chinese_test.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/HUANGWEIJIE/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    527\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/HUANGWEIJIE/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/HUANGWEIJIE/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_options_with_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/HUANGWEIJIE/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    745\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/HUANGWEIJIE/anaconda/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1119\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3246)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:6111)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: File b'new_CSV_Data/raw_chinese_train.csv' does not exist"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda, Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D, MaxPooling1D\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "import re\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import text\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# 初始变量\n",
    "txt_train = ''\n",
    "txt_test = ''\n",
    "txt_all = ''\n",
    "\n",
    "docs_train = []\n",
    "labels_train = []\n",
    "docs_test = []\n",
    "labels_test = []\n",
    "nb_classes = 5\n",
    "maxlen = 250\n",
    "\n",
    "datatrain = pd.read_csv(\"../new_CSV_Data/raw_chinese_train.csv\", header=0)\n",
    "datatest = pd.read_csv(\"../new_CSV_Data/raw_chinese_test.csv\", header=0)\n",
    "\n",
    "\n",
    "# 创建 len(docs)个, 1 * maxlen 的矩阵\n",
    "X_train = np.ones((datatrain.shape[0], maxlen), dtype = np.int64) * 0\n",
    "X_test = np.ones((datatest.shape[0], maxlen), dtype = np.int64) * 0\n",
    "\n",
    "\n",
    "\n",
    "#解压缩数据到相应的docs和labels List内\n",
    "print('zipping the training data:')\n",
    "epoch = 0\n",
    "for label, content in zip(datatrain.classes, datatrain.content):\n",
    "    content = re.sub(\"[^\\u4E00-\\u9FFF＼，＼．＼！＼？]\", \" \", content)\n",
    "    docs_train.append(content)\n",
    "    labels_train.append(label)\n",
    "    epoch = epoch + 1\n",
    "    if (epoch % 20000 == 0):\n",
    "        print('zipping the training data:', epoch)\n",
    "print('Success, zipping the training data!')\n",
    "\n",
    "print('There are training set:', datatrain.shape[0])\n",
    "\n",
    "print('zipping the test data:')\n",
    "epoch = 0\n",
    "for label, content in zip(datatest.classes, datatest.content):\n",
    "    content = re.sub(\"[^\\u4E00-\\u9FFF\\,\\.\\!\\?]\", \" \", content)\n",
    "    docs_test.append(content)\n",
    "    labels_test.append(label)\n",
    "    epoch = epoch + 1\n",
    "    if (epoch % 20000 == 0):\n",
    "        print('zipping the test data:', epoch)\n",
    "print('Success, zipping the test data!')\n",
    "\n",
    "print('There are test set:', datatest.shape[0])\n",
    "\n",
    "\n",
    "print('Construct the Dictionary!')\n",
    "#用Set获取具体的中文数据集的Dict\n",
    "for doc in docs_train:\n",
    "    for text in doc:\n",
    "        txt_train = txt_train + text  \n",
    "        \n",
    "for doc in docs_test:\n",
    "    for text in doc:\n",
    "        txt_test = txt_test + text  \n",
    "        \n",
    "chars_train = set(txt_train)\n",
    "chars_test = set(txt_test)\n",
    "chars_all = set(txt_train + txt_test)\n",
    "print('total chars:', len(chars_train))\n",
    "print('total chars:', len(chars_test))\n",
    "print('total chars:', len(chars_all))\n",
    "\n",
    "input_dim = len(chars_all)\n",
    "\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars_all))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars_all))\n",
    "\n",
    "\n",
    "#用获得的Dict来做1 of m encoding\n",
    "\n",
    "print('Doing one hot encoding for training set:')\n",
    "# One-Hot encoding 另外应该是反过来进行 encode 的,,稀疏部分用0代替\n",
    "for i, doc in enumerate(docs_train):\n",
    "    # 倒着数后面的maxlen个数字,但是输出顺序不变\n",
    "    for t, char in enumerate(doc[-maxlen:]):\n",
    "                X_train[i, (maxlen-1-t)] = char_indices[char]\n",
    "print('Success!')\n",
    "\n",
    "print('Doing one hot encoding for test set:')\n",
    "# One-Hot encoding 另外应该是反过来进行 encode 的,,稀疏部分用-1代替\n",
    "for i, doc in enumerate(docs_test):\n",
    "    # 倒着数后面的maxlen个数字,但是输出顺序不变\n",
    "    for t, char in enumerate(doc[-maxlen:]):\n",
    "                X_test[i, (maxlen-1-t)] = char_indices[char]\n",
    "print('Success!')\n",
    "\n",
    "\n",
    "# 开始对train和test的Y进行处理\n",
    "Y_train = np.array(labels_train)\n",
    "print('Convert class vector to binary class matrix (for use with categorical_crossentropy)')\n",
    "print(nb_classes, 'classes in the dataset')\n",
    "Y_train = np_utils.to_categorical(Y_train, nb_classes)\n",
    "print('Success!')\n",
    "\n",
    "Y_test = np.array(labels_test)\n",
    "print('Convert class vector to binary class matrix (for use with categorical_crossentropy)')\n",
    "print(nb_classes, 'classes in the dataset')\n",
    "Y_test = np_utils.to_categorical(Y_test, nb_classes)\n",
    "print('Success!')\n",
    "\n",
    "\n",
    "\n",
    "print(\"All of the pre-processde work is done.\")\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# we start off with an efficient embedding layer which maps\n",
    "# our vocab indices into embedding_dims dimensions\n",
    "model.add(Embedding(input_dim, output_dim = 400, input_length = maxlen, init = 'he_normal', W_regularizer=l2(0.01)) )\n",
    "\n",
    "# we add a Convolution1D, which will learn nb_filter\n",
    "# word group filters of size filter_length:\n",
    "model.add(Convolution1D(nb_filter = 128, filter_length = 3, W_regularizer=l2(0.01),  init = 'he_normal', border_mode='same', activation='relu', subsample_length=1))\n",
    "model.add(Convolution1D(nb_filter = 128, filter_length = 3, W_regularizer=l2(0.01),  init = 'he_normal', border_mode='same', activation='relu', subsample_length=1))\n",
    "model.add(Convolution1D(nb_filter = 128, filter_length = 3, W_regularizer=l2(0.01),  init = 'he_normal', border_mode='same', activation='relu', subsample_length=1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# we use max pooling:\n",
    "model.add(MaxPooling1D(pool_length = model.output_shape[1]))\n",
    "#model.add(MaxPooling1D(pool_length = 2))\n",
    "#print(model.output_shape[1], \"pooling shape\")\n",
    "# We flatten the output of the conv layer,\n",
    "# so that we can add a vanilla dense layer:\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(100))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We add a vanilla hidden layer:\n",
    "model.add(Dense(100))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
    "model.add(Dense(5))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "checkpointers = ModelCheckpoint(\"parameters/weights.{epoch:02d}-{val_acc:.4f}.hdf5\", monitor='val_acc', verbose=0, save_best_only=False, mode='auto')\n",
    "\n",
    "#model.load_weights(\"parameters/weights.39-0.32.hdf5\")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, batch_size = 64, nb_epoch = 20, validation_data=(X_test, Y_test), callbacks = [checkpointers])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
